{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2404dd-855d-4917-9ef4-239bb4e165ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulação de Dados com DataFrames no PySpark - Operação READ\n",
    "\n",
    "**Objetivo:** Nesta seção, você aprenderá como carregar dados utilizando DataFrames no PySpark. Os DataFrames são uma das abstrações mais poderosas do Apache Spark, fornecendo uma API de alto nível para o processamento eficiente de dados distribuídos.\n",
    "\n",
    "#### Criação de DataFrames\n",
    "\n",
    "##### A partir de Arquivos (CSV, JSON, Parquet, etc.)\n",
    "\n",
    "O PySpark suporta a leitura de múltiplos formatos de arquivos, como CSV, JSON, Parquet, ORC, Avro, entre outros. A seguir, exploramos as opções disponíveis para cada formato, com exemplos detalhados para uma compreensão completa.\n",
    "\n",
    "---\n",
    "\n",
    "### Carregando Dados de Arquivos CSV\n",
    "\n",
    "Documentação de referência: [CSV Data Source](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "O Spark oferece uma variedade de opções de parametrização ao ler arquivos CSV, facilitando o tratamento de arquivos com diferentes formatos e estruturas.\n",
    "\n",
    "**Opções Comuns de Parametrização para CSV:**\n",
    "- **`header`**: Especifica se o arquivo CSV tem um cabeçalho (`True` ou `False`).\n",
    "- **`sep`**: Define o delimitador de colunas (ex: `','`, `';'`).\n",
    "- **`inferSchema`**: Infere automaticamente o tipo de dado das colunas (`True` ou `False`).\n",
    "- **`nullValue`**: Define valores específicos no CSV que devem ser interpretados como `null`.\n",
    "- **`quote`**: Define o caractere usado para aspas (ex: `'\"'`).\n",
    "- **`escape`**: Define o caractere usado para escapar aspas (ex: `'\\\\'`).\n",
    "- **`encoding`**: Especifica a codificação do arquivo (ex: `'UTF-8'`).\n",
    "\n",
    "**Exemplo Prático:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9126e695-d626-4fba-a9c7-27b677b5219b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create volume if not exists `laboratorio-spark`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d08d93-251a-47a2-b4fb-af3f7bb9d9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "wget -O /Volumes/workspace/default/laboratorio-spark/airtravel.csv https://raw.githubusercontent.com/rafael-negrao/laboratorio-spark/main/dados/airtravel.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f34ea71-edc6-4d12-acd4-690e42462b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = \"/Volumes/workspace/default/laboratorio-spark/airtravel.csv\"\n",
    "\n",
    "# Lendo o CSV com opções de parametrização\n",
    "df_csv = spark.read.csv(\n",
    "    file, \n",
    "    header=True,         # Indica que o arquivo tem cabeçalho\n",
    "    sep=',',             # Delimitador das colunas\n",
    "    inferSchema=True,    # Infere automaticamente os tipos das colunas\n",
    "    nullValue='NA',      # Define valores nulos\n",
    "    quote='\"',           # Define o caractere de aspas\n",
    "    escape='\\\\',         # Caractere de escape\n",
    "    encoding='UTF-8'     # Codificação do arquivo\n",
    ")\n",
    "display(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a61da84e-31ef-4c0b-9bdd-0a84cd7dd47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Carregando Dados de Arquivos JSON\n",
    "\n",
    "Documentação de referência: [JSON Data Source](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "\n",
    "O Spark suporta dois formatos principais de JSON: **inline** (todas as entradas em uma única linha) e **multilinha** (entradas separadas por linhas).\n",
    "\n",
    "#### JSON Inline\n",
    "\n",
    "**Exemplo de Arquivo JSON Inline:**\n",
    "```json\n",
    "{\"name\": \"Alice\", \"age\": 30}\n",
    "{\"name\": \"Bob\", \"age\": 25}\n",
    "```\n",
    "\n",
    "**Opções Comuns de Parametrização para JSON Inline:**\n",
    "- **`multiline`**: Deve ser `False` (padrão) para JSON inline.\n",
    "- **`primitivesAsString`**: Interpreta tipos primitivos como strings (`True` ou `False`).\n",
    "- **`allowSingleQuotes`**: Permite o uso de aspas simples para delimitar strings (`True` ou `False`).\n",
    "- **`mode`**: Define o comportamento em caso de erros (`PERMISSIVE`, `DROPMALFORMED`, `FAILFAST`).\n",
    "\n",
    "**Exemplo Prático:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe9cc31-08ea-484b-858e-7d7ffc294644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help(\"put\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f524e235-d8e9-4721-a46e-5175e9f50f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = \"/Volumes/workspace/default/laboratorio-spark/exemplo.json\"\n",
    "\n",
    "dbutils.fs.put(\n",
    "    file, \n",
    "    \"\"\"{\"name\": \"Alice\", \"age\": 30}\n",
    "{\"name\": \"Bob\", \"age\": 25}\"\"\",\n",
    "    overwrite=True)\n",
    "\n",
    "# Lendo JSON Inline\n",
    "df_json_inline = spark.read.json(\n",
    "    file, \n",
    "    multiLine=False,            # JSON inline\n",
    "    primitivesAsString=True,    # Converte primitivos para string\n",
    "    allowSingleQuotes=True,     # Permite aspas simples\n",
    "    mode='PERMISSIVE'           # Modo permissivo para erros\n",
    ")\n",
    "display(df_json_inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b2e508-63e5-48e8-b82e-3f62acdab81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### JSON Multilinha\n",
    "\n",
    "**Exemplo de JSON Multilinha:**\n",
    "```json\n",
    "[\n",
    "  {\"name\": \"Alice\", \"age\": 30},\n",
    "  {\"name\": \"Bob\", \"age\": 25}\n",
    "]\n",
    "```\n",
    "\n",
    "**Opções Comuns de Parametrização para JSON Multilinha:**\n",
    "- **`multiline`**: Deve ser `True` para JSON multilinha.\n",
    "- **`primitivesAsString`**, **`allowSingleQuotes`**, **`mode`**: As mesmas opções que o JSON inline.\n",
    "\n",
    "**Exemplo Prático:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc68b3c-ade8-484a-a66d-41a1db1041b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = \"/Volumes/workspace/default/laboratorio-spark/exemplo-multilinha.json\"\n",
    "\n",
    "dbutils.fs.put(\n",
    "    file, \n",
    "    \"\"\"[\n",
    "  {\"name\": \"Alice\", \"age\": 30},\n",
    "  {\"name\": \"Bob\", \"age\": 25}\n",
    "]\"\"\",\n",
    "    overwrite=True)\n",
    "\n",
    "\n",
    "# Lendo JSON Multilinha\n",
    "df_json_multiline = spark.read.json(\n",
    "    file, \n",
    "    multiLine=True,             # JSON Multilinha\n",
    "    primitivesAsString=True,    # Converte primitivos para string\n",
    "    allowSingleQuotes=True,     # Permite aspas simples\n",
    "    mode='PERMISSIVE'           # Modo permissivo para erros\n",
    ")\n",
    "display(df_json_multiline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7331fe78-90dd-4ff4-83c3-3c1df2c5b106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Carregando Dados de Arquivos Parquet\n",
    "\n",
    "Documentação de referência: [Parquet Data Source](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n",
    "\n",
    "O formato Parquet é otimizado para leitura rápida e eficiente, especialmente para grandes volumes de dados.\n",
    "\n",
    "**Opções Comuns de Parametrização para Parquet:**\n",
    "- **`mergeSchema`**: Combina esquemas de múltiplos arquivos (`True` ou `False`).\n",
    "- **`spark.sql.parquet.filterPushdown`**: Controla o pushdown de filtros para otimizar consultas (habilitado por padrão).\n",
    "- **`spark.sql.parquet.enableVectorizedReader`**: Habilita a leitura vetorizada para melhorar a performance (habilitado por padrão).\n",
    "\n",
    "**Exemplo Prático:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fc0797-c155-4729-9dc7-861efbd7466b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diretorio = \"/Volumes/workspace/default/laboratorio-spark/example_parquet\"\n",
    "\n",
    "# Criando um DataFrame de exemplo e salvando como Parquet\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "df.write.mode(\"overwrite\").parquet(diretorio)\n",
    "\n",
    "# Lendo o arquivo Parquet com opções\n",
    "df_parquet = (\n",
    "    spark\n",
    "        .read\n",
    "        .option(\"mergeSchema\", \"true\") # Combina esquemas se múltiplos arquivos estiverem presentes\n",
    "        .parquet(diretorio)\n",
    ") \n",
    "\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433553f2-e450-4a24-bba6-528c40a3332d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lendo o arquivo Parquet com opções\n",
    "df_parquet2 = (\n",
    "    spark\n",
    "        .read\n",
    "        .format(\"parquet\")\n",
    "        .option(\"mergeSchema\", \"true\") # Combina esquemas se múltiplos arquivos estiverem presentes\n",
    "        .load(diretorio)\n",
    ") \n",
    "display(df_parquet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f0f405-5089-4e37-8d55-362002f00b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Carregando Dados de Arquivos Text\n",
    "\n",
    "Documentação de referência: [Text Data Source](https://spark.apache.org/docs/latest/sql-data-sources-text.html)\n",
    "\n",
    "O formato de texto é simples e pode ser útil para dados não estruturados ou entradas de logs.\n",
    "\n",
    "**Opções Comuns de Parametrização para Text:**\n",
    "- **`wholetext`**: Lê o arquivo inteiro como uma única entrada (`True` ou `False`).\n",
    "- **`lineSep`**: Define o separador de linha (ex: `'\\n'`).\n",
    "\n",
    "**Exemplo Prático:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64cf5f8-8844-4c35-ba3e-c9c2bfb69008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file = \"/Volumes/workspace/default/laboratorio-spark/exemplo-olamundo.txt\"\n",
    "\n",
    "dbutils.fs.put(\n",
    "    file, \n",
    "    \"\"\"Hello World\\nWelcome to PySpark\"\"\",\n",
    "    overwrite=True)\n",
    "\n",
    "# Lendo o arquivo de texto com opções\n",
    "df_text = spark.read.text(\n",
    "    file, \n",
    "    wholetext=False,     # Lê o arquivo linha por linha\n",
    "    lineSep='\\n',        # Define o separador de linha\n",
    ")\n",
    "display(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f66145-cfbf-4285-93e6-276fb0a740f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`[INFO]: FIM DO NOTEBOOK`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24cde29-b7b8-4bd1-a80b-d9bf5c7f2686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5123364838639515,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02.00.manipulacao-de-dados-read",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
