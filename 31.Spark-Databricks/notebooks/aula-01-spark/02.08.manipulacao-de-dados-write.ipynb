{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86dc5cb-c62c-4c3a-a40a-08c33e79a540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "> **Rodar este notebook no ambiente pago da databricks**\n",
    "\n",
    "### Manipulação de Dados com DataFrames no PySpark - write\n",
    "\n",
    "A função **`write`** em PySpark é usada para salvar um DataFrame em um arquivo ou tabela. A estrutura geral é:\n",
    "\n",
    "```\n",
    "(\n",
    "dataframe\n",
    "    .write\n",
    "    .option(\"option_name\", \"option_value\")\n",
    "    .format(\"file_format\")\n",
    "    .mode(\"mode\")\n",
    "    .save(\"path\")\n",
    ")\n",
    "```\n",
    "\n",
    "#### Estrutura Geral\n",
    "\n",
    "1. **`df.write`**: Inicia a operação de escrita do DataFrame.\n",
    "2. **`option(\"option_name\", \"option_value\")`**: Define opções específicas para o formato de saída (ex.: `header` para CSV, `compression` para Parquet).\n",
    "3. **`format(\"file_format\")`**: Define o formato do arquivo de saída (ex.: `\"csv\"`, `\"json\"`, `\"parquet\"`, etc.).\n",
    "4. **`mode(\"mode\")`**: Define o comportamento quando os dados já existem no destino (ex.: `\"overwrite\"`, `\"append\"`).\n",
    "5. **`save(\"path\")`**: Define o caminho onde os dados serão salvos.\n",
    "\n",
    "#### Tipos de mode\n",
    "\n",
    "- `overwrite`: Sobrescreve o arquivo ou tabela existente.\n",
    "- `append`: Adiciona novos dados ao arquivo ou tabela existente.\n",
    "- `ignore`: Não realiza nenhuma ação se o arquivo ou tabela já existir.\n",
    "- `error` (padrão): Lança um erro se o arquivo ou tabela já existir.\n",
    "\n",
    "#### Salvando como CSV\n",
    "\n",
    "**Preparando os dados**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82ccb1e-136d-4ca5-8dc3-f3c43388e465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a03656c-a6fb-4d23-b0e9-caf5a10ea278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Massa de dados\n",
    "data = [\n",
    "    (1, \"Alice\", 25, 50000),\n",
    "    (2, \"Bob\", 30, 60000),\n",
    "    (3, \"Charlie\", 35, 70000),\n",
    "    (4, \"David\", 40, 80000),\n",
    "    (5, \"Eve\", 45, 90000)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\"]\n",
    "\n",
    "# Criar o DataFrame\n",
    "massa_dados_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar os dados\n",
    "print(\"Exibindo o DataFrame:\")\n",
    "display(massa_dados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3b30bb-8ee4-44f7-b0e2-f2283a47b543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Salvar DataFrame como CSV**\n",
    "\n",
    "> O caminho default do spark-warehouse é: /user/hive/warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4ec12d-09d4-4a79-b985-b005fd5a01ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "massa_dados_df\n",
    "    .repartition(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .save(f\"/Volumes/workspace/default/laboratorio-spark//exemplo_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc7abc-3b24-47eb-9e69-44018620b8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"/Volumes/workspace/default/laboratorio-spark/exemplo_csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28bb7bbd-a5fc-46c8-89bc-d4333844a9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Documentação de referência: https://spark.apache.org/docs/3.5.2/sql-data-sources-csv.html\n",
    "\n",
    "- **Opções**:\n",
    "  - `\"header\"`: Especifica se a primeira linha será o cabeçalho.\n",
    "  - `\"delimiter\"`: Define o delimitador de campo (padrão é ,).\n",
    "\n",
    "#### Salvando como JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b09fda-0894-4680-abd6-71ecac0534ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    massa_dados_df\n",
    "    .write\n",
    "    .format(\"json\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"gzip\")\n",
    "    .save(f\"/Volumes/workspace/default/laboratorio-spark/exemplo_json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a15dd3-9d5d-4ec0-a5e2-b306f84dfb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"/Volumes/workspace/default/laboratorio-spark//exemplo_json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6c0c8e-1459-432c-857a-7c316f8b1121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Documentação de referência: https://spark.apache.org/docs/3.5.2/sql-data-sources-json.html\n",
    "\n",
    "- **Opções**:\n",
    "  - `\"compression\"`: Define o tipo de compressão (ex.: `\"gzip\"`, `\"bzip2\"`, `\"snappy\"`).\n",
    "\n",
    "#### Salvando como Parquet\n",
    "\n",
    "O formato **Parquet** é otimizado para análise de grandes volumes de dados. Ele suporta particionamento de dados para melhorar a eficiência das consultas.\n",
    "\n",
    "**Preparando a massa de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6833652b-5c79-4ae1-8372-3ba893940547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Funções auxiliares para gerar dados aleatórios\n",
    "def random_name():\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\"]\n",
    "    return random.choice(names)\n",
    "\n",
    "def random_age():\n",
    "    return random.randint(20, 60)\n",
    "\n",
    "def random_salary():\n",
    "    return random.randint(30000, 100000)\n",
    "\n",
    "def random_year():\n",
    "    return random.choice([2021, 2022, 2023, 2024])\n",
    "\n",
    "def random_month():\n",
    "    return random.randint(1, 12)\n",
    "\n",
    "def random_day():\n",
    "    return random.randint(1, 28)\n",
    "\n",
    "# Gerar massa de dados\n",
    "data = [(i, random_name(), random_age(), random_salary(), random_year(), random_month(), random_day()) for i in range(1, 2001)]\n",
    "\n",
    "# Definir as colunas\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\", \"year\", \"month\", \"day\"]\n",
    "\n",
    "# Criar o DataFrame\n",
    "parquet_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar os dados\n",
    "print(\"Exibindo os primeiros registros do DataFrame:\")\n",
    "display(parquet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69941d73-b4c6-404c-954e-865b4f173609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    parquet_df\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"/Volumes/workspace/default/laboratorio-spark/exemplo_parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e97314-856a-42b3-842d-fbcd49789e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"/Volumes/workspace/default/laboratorio-spark/exemplo_parquet/year=2024/month=9/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f87c41-5f1b-4cc5-80e6-c43a6a146db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Opções**:\n",
    "    - `partitionBy`: Divide os dados em subdiretórios com base nos valores de uma ou mais colunas. Isso melhora a leitura seletiva dos dados.\n",
    "        - Exemplo: `partitionBy(\"year\", \"month\")` criará subdiretórios como `year=2024/month=09/`.\n",
    "    - `bucketBy`: Agrupa os dados em \"buckets\" com base em uma ou mais colunas. Deve ser usado com **`saveAsTable`**.\n",
    "        - Exemplo: `bucketBy(10, \"id\")` agrupa os dados em 10 buckets com base na coluna `id`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5be35c-e2d9-4557-a360-0d0a4e823b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Salvando como tabela interna\n",
    "\n",
    "Uma **tabela interna** é gerenciada pelo Spark e armazenada no espaço de armazenamento do Spark. Essas tabelas estão geralmente no diretório padrão do warehouse do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904956ca-1191-4b98-8069-aa96d4775274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    parquet_df\n",
    "    .write\n",
    "    .format(\"delta\") # no ambiente serverless nao suporta parquet puro\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    # .bucketBy(100, \"id\") # quando o formato é parquet\n",
    "    .saveAsTable(\"default.internal_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9f8392-fd4f-4475-b7ef-1de5a7fa8dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table default.internal_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ae7857-f39f-4c8b-baf7-b73a560b5451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Opções**:\n",
    "    - `partitionBy`: Divide os dados em subdiretórios com base nos valores de uma ou mais colunas. Isso melhora a leitura seletiva dos dados.\n",
    "    - Exemplo: `partitionBy(\"year\", \"month\")` criará subdiretórios como `year=2024/month=09/`.\n",
    "    - `bucketBy`: Agrupa os dados em \"buckets\" com base em uma ou mais colunas. Deve ser usado com **`saveAsTable`**.\n",
    "    - Exemplo: `bucketBy(10, \"id\")` agrupa os dados em 10 buckets com base na coluna `id`.\n",
    "\n",
    "#### Salvando como Tabela Externa\n",
    "\n",
    "Uma **tabela externa** permite que os dados sejam armazenados em um caminho especificado pelo usuário. O Spark gerencia apenas os metadados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75139cd4-7426-482e-9e9c-4754a40ef18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    parquet_df\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"path\", f\"/Volumes/workspace/default/laboratorio-spark/external_table__\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .saveAsTable(\"default.external_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6521689-f4d9-42a5-80bb-0d9ffec5efce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Nota**: Aqui o caminho para a tabela é especificado com a opção `\"path\"`, o que permite que os dados sejam armazenados externamente ao warehouse do Spark.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4ed94e-f475-41dc-ae5b-1db59d18b2d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"/Volumes/workspace/default/laboratorio-spark/external_table__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702c7630-3d46-423c-b98f-6ac001f6312b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table external_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ae4012-db44-4e75-929a-3e0db6b6b845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"/Volumes/workspace/default/laboratorio-spark/external_table__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2976e4ff-7de3-4d3f-ba0e-c414d5b4b26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`[INFO]: FIM DO NOTEBOOK`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5123364838639522,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02.08.manipulacao-de-dados-write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
