{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697e53ff-16ad-4972-9e99-852e47d66bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulação de Dados com DataFrames no PySpark - UDF\n",
    "\n",
    "Nesta explicação, vamos detalhar como utilizar **Spark UDF** e **Pandas UDF**, abordando exemplos práticos e suas parametrizações. Veremos como aplicar UDFs com diferentes tipos de entradas e saídas, incluindo estruturas complexas.\n",
    "\n",
    "---\n",
    "\n",
    "### Spark UDF (User Defined Function)\n",
    "\n",
    "As **Spark UDFs** permitem que você crie funções personalizadas em Python e as aplique a colunas de DataFrames no PySpark. As UDFs tradicionais funcionam linha a linha e podem ser aplicadas a uma ou mais colunas.\n",
    "\n",
    "#### Exemplo 1: UDF com Duas Entradas e Uma Saída (Simples)\n",
    "\n",
    "Neste exemplo, vamos criar uma UDF para concatenar duas colunas de strings e retornar o resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6591f9-ba50-44df-9584-f54eb865a5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0955324-c077-40cd-ad3b-32933a0dd9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Criar um DataFrame de exemplo\n",
    "data = [(\"Alice\", \"Smith\"), (\"Bob\", \"Johnson\"), (\"Charlie\", \"Brown\")]\n",
    "schema = [\"first_name\", \"last_name\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Definir uma função Python para concatenar dois strings\n",
    "def concat_names(first_name, last_name):\n",
    "    return first_name + \" \" + last_name\n",
    "\n",
    "# Registrar a função como uma UDF\n",
    "concat_udf = udf(concat_names, StringType())\n",
    "\n",
    "# Aplicar a UDF ao DataFrame\n",
    "df_with_full_name = df.withColumn(\"full_name\", concat_udf(df[\"first_name\"], df[\"last_name\"]))\n",
    "\n",
    "print(\"DataFrame com Nome Completo:\")\n",
    "display(df_with_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1102a5-c5c8-4cd0-ba6c-9fb118caa709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Exemplo 2: UDF com Duas Entradas e Uma Saída (Struct)\n",
    "\n",
    "Neste exemplo, a UDF retorna uma **struct** (estrutura) como saída. Vamos combinar duas colunas e retornar uma estrutura com duas chaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2e2e15-4eb6-4d6e-afe6-6a033bdac6c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Criar um DataFrame de exemplo\n",
    "data = [(\"Alice\", \"Smith\"), (\"Bob\", \"Johnson\"), (\"Charlie\", \"Brown\")]\n",
    "schema = [\"first_name\", \"last_name\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Definir uma função Python para criar uma estrutura de dados\n",
    "def create_struct(first_name, last_name):\n",
    "    return {\"first\": first_name, \"last\": last_name}\n",
    "\n",
    "# Definir o esquema da struct\n",
    "struct_schema = StructType([\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Registrar a função como uma UDF\n",
    "struct_udf = udf(create_struct, struct_schema)\n",
    "\n",
    "# Aplicar a UDF ao DataFrame\n",
    "df_with_struct = df.withColumn(\"name_struct\", struct_udf(df[\"first_name\"], df[\"last_name\"]))\n",
    "\n",
    "print(\"DataFrame com Estrutura (Struct):\")\n",
    "display(df_with_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4af2bc-e22c-4318-b5e8-3032325aa605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Explicação do código:**\n",
    "- A função `create_struct` retorna um dicionário Python, que o Spark converte em uma estrutura (`struct`).\n",
    "- O esquema da estrutura é definido usando `StructType` com dois campos: `first` e `last`.\n",
    "- A UDF é aplicada ao DataFrame para gerar a nova coluna `name_struct` com uma estrutura contendo o primeiro e último nome.\n",
    "\n",
    "---\n",
    "\n",
    "### Pandas UDF (Vectorized UDF)\n",
    "\n",
    "As **Pandas UDFs** utilizam a integração do **Apache Arrow** para realizar operações vetorizadas, permitindo que os dados sejam processados em blocos como **Pandas Series** e **DataFrames**, melhorando a eficiência em relação às UDFs tradicionais.\n",
    "\n",
    "#### Como Parametrizar o Spark para Pandas UDF\n",
    "\n",
    "Antes de usar Pandas UDFs, é importante ativar o suporte ao Apache Arrow no Spark para garantir a máxima eficiência.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46596655-0306-4813-81b4-8d2553135762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NAO PRECISA CONFIGURAR AMBIENTE SERVERLESS\n",
    "\n",
    "# Ativar o uso do Apache Arrow\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2c98c0-5202-46e0-9252-4d98311bff0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Pandas UDF: Series to Series\n",
    "\n",
    "**Series to Series** é uma UDF onde uma ou mais colunas são passadas como entrada e uma nova coluna (Series) é retornada.\n",
    "\n",
    "##### Exemplo 1: Pandas UDF com Uma Entrada\n",
    "\n",
    "Neste exemplo, criamos uma Pandas UDF que converte uma coluna de nomes para maiúsculas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a199f0c8-a94d-4065-afe7-ef1c3b9867ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "# Definir uma Pandas UDF para converter texto em maiúsculas\n",
    "@pandas_udf(StringType())\n",
    "def to_uppercase(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "# Aplicar a Pandas UDF ao DataFrame\n",
    "df_upper = df.withColumn(\"name_upper\", to_uppercase(df[\"first_name\"]))\n",
    "\n",
    "print(\"DataFrame com Nome em Maiúsculas:\")\n",
    "display(df_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e49e5e1-05f2-4970-9a9f-06b6b8db181e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Exemplo 2: Pandas UDF com Duas Entradas\n",
    "\n",
    "Vamos agora criar uma Pandas UDF que concatena duas colunas (primeiro nome e sobrenome) e retorna o nome completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3541c8-19b5-482d-a69a-b933c2bc380f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(StringType())\n",
    "def concat_names(first_name: pd.Series, last_name: pd.Series) -> pd.Series:\n",
    "    return first_name + \" \" + last_name\n",
    "\n",
    "# Aplicar a Pandas UDF ao DataFrame\n",
    "df_full_name = df.withColumn(\"full_name\", concat_names(df[\"first_name\"], df[\"last_name\"]))\n",
    "\n",
    "print(\"DataFrame com Nome Completo:\")\n",
    "display(df_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39dc83c7-dba7-44f4-97dc-9d0dfd1b7b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Pandas UDF: Series to Scalar\n",
    "\n",
    "**Series to Scalar** é usada para realizar operações agregadas, onde a função recebe uma coluna de dados (Series) e retorna um valor escalar único (por exemplo, média, soma).\n",
    "\n",
    "**Exemplo:** Calcular a média de uma coluna numérica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c7cb74-d577-4757-b85d-c3cb29471dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Criar uma massa de dados com valores numéricos\n",
    "data = [\n",
    "    (1, 100.0),\n",
    "    (2, 200.0),\n",
    "    (3, 300.5),\n",
    "    (4, 150.25),\n",
    "    (5, 250.75),\n",
    "    (6, 325.0),\n",
    "    (7, 175.4)\n",
    "]\n",
    "\n",
    "# Definir o esquema do DataFrame\n",
    "columns = [\"id\", \"value\"]\n",
    "\n",
    "# Criar o DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar a massa de dados inicial\n",
    "print(\"Massa de Dados:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e17136-c889-41d9-9e2c-d35c31e8cb1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definir uma Pandas UDF para calcular a média\n",
    "@pandas_udf(DoubleType())\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "# Aplicar a UDF ao DataFrame para calcular a média\n",
    "df_mean = df.select(mean_udf(df[\"value\"]).alias(\"mean_value\"))\n",
    "\n",
    "print(\"Média dos Valores:\")\n",
    "display(df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f8b1a6-4662-49f7-8d9b-6ec5016f7565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Pandas UDF: Grouped Map\n",
    "\n",
    "**Grouped Map** permite que você agrupe os dados por uma ou mais colunas e aplique uma função personalizada a cada grupo. A função recebe um **Pandas DataFrame** para cada grupo e retorna um novo DataFrame.\n",
    "\n",
    "**Exemplo:** Calcular a soma e a média dos valores dentro de cada grupo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc717df-80c5-4906-a643-5122fc8f687d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Criar uma massa de dados com grupos e valores numéricos\n",
    "data = [\n",
    "    (\"A\", 10),\n",
    "    (\"A\", 20),\n",
    "    (\"A\", 30),\n",
    "    (\"B\", 15),\n",
    "    (\"B\", 25),\n",
    "    (\"B\", 35),\n",
    "    (\"C\", 50),\n",
    "    (\"C\", 60)\n",
    "]\n",
    "\n",
    "# Definir o esquema do DataFrame\n",
    "columns = [\"group\", \"value\"]\n",
    "\n",
    "# Criar o DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar a massa de dados inicial\n",
    "print(\"Massa de Dados:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b35b63f-d9e3-4875-a519-525532a0abba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir o esquema de saída\n",
    "schema_out = StructType([\n",
    "    StructField(\"group\", StringType()),\n",
    "    StructField(\"sum_value\", IntegerType()),\n",
    "    StructField(\"mean_value\", DoubleType())\n",
    "])\n",
    "\n",
    "# Definir uma função de Pandas que será aplicada por grupo\n",
    "def group_operations(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    sum_value = pdf[\"value\"].sum()\n",
    "    mean_value = pdf[\"value\"].mean()\n",
    "    return pd.DataFrame({\n",
    "        \"group\": [pdf[\"group\"].iloc[0]],\n",
    "        \"sum_value\": [sum_value],\n",
    "        \"mean_value\": [mean_value]\n",
    "    })\n",
    "\n",
    "# Aplicar a função com applyInPandas\n",
    "df_grouped = df.groupBy(\"group\").applyInPandas(group_operations, schema=schema_out)\n",
    "\n",
    "# Mostrar o resultado\n",
    "print(\"Soma e Média por Grupo:\")\n",
    "display(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08441d3-8229-4f10-ab39-41948d2f1473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`[INFO]: Fim Notebook`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.04.manipulacao-de-dados-udf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
