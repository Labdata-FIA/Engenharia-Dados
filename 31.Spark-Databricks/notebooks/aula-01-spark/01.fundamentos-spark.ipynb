{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f351b36-9910-4de7-816e-5c52399ea2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fundamentos do Spark\n",
    "\n",
    "#### 1.1. Introdução ao Apache Spark\n",
    "\n",
    "##### O que é Apache Spark?\n",
    "- Apache Spark é um framework de código aberto para computação distribuída que facilita o processamento de grandes volumes de dados. Ele foi projetado para ser rápido e eficiente, permitindo o processamento de dados em larga escala, tanto em modo batch (lote) quanto em tempo real.\n",
    "\n",
    "##### Arquitetura do Spark\n",
    "- **Driver:** O Driver é responsável por coordenar o trabalho no Spark. Ele executa o código principal do usuário, cria o SparkContext, e distribui as tarefas aos executores.\n",
    "- **Executors:** São os processos responsáveis por executar as tarefas distribuídas pelo Driver. Eles executam operações nos dados e armazenam resultados intermediários na memória ou disco conforme necessário.\n",
    "- **Cluster Manager:** O Spark pode ser executado em diferentes modos, como standalone, sobre Apache Hadoop YARN, Mesos, ou Kubernetes, com o Cluster Manager gerenciando os recursos necessários para as tarefas.\n",
    "\n",
    "##### Componentes Principais do Spark\n",
    "- **Spark Core:** O núcleo do Spark, que inclui o suporte para processamento de tarefas, agendamento, e operações básicas sobre RDDs.\n",
    "- **Spark SQL:** Permite trabalhar com dados estruturados usando SQL ou DataFrames, facilitando a manipulação e consulta de dados com uma sintaxe familiar.\n",
    "- **Spark Streaming:** Facilita o processamento de fluxos de dados em tempo real usando micro-batching.\n",
    "- **MLlib:** Biblioteca de machine learning distribuída, que oferece algoritmos e utilitários para tarefas de aprendizado de máquina.\n",
    "- **GraphX:** Biblioteca para computação em grafos, permitindo análise de dados que têm uma estrutura de grafo.\n",
    "\n",
    "#### 1.2. Conceitos Fundamentais\n",
    "\n",
    "##### Resilient Distributed Datasets (RDDs)\n",
    "- **RDD:** É a principal abstração do Spark. Um RDD é uma coleção distribuída e imutável de objetos que pode ser processada em paralelo. RDDs são resilientes, o que significa que eles podem se recuperar automaticamente de falhas.\n",
    "\n",
    "**Transformações e Ações:**\n",
    "- **Transformações:** Operações que retornam novos RDDs, como `map`, `filter`, e `flatMap`. Transformações são executadas de forma preguiçosa (lazy), ou seja, elas apenas definem uma sequência de operações que serão executadas quando uma ação for chamada.\n",
    "\n",
    "##### Exemplos de Transformações Comuns\n",
    "\n",
    "**Exemplo de Código Básico:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df3862c-35f5-4a9f-b332-471f59ea89e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2387b97b-16c9-41bc-a39c-9f1003eea664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação de um RDD simples\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Realizando uma transformação (map) e uma ação (collect)\n",
    "squared_rdd = rdd.map(lambda x: x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0501dc7-7e19-442a-ad4d-6632118b31f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(squared_rdd.collect())  # Output: [1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6d2869-bdec-48ce-b925-9ac2db05cc74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Map:** Aplica uma função a cada elemento do RDD e retorna um novo RDD com os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddafe4a-54e3-469a-98ae-99fd7a51796c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Elevar ao quadrado cada elemento do RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "print(squared_rdd.collect())  # Output: [1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9d06fc-91ce-4e7f-816d-c29b7302e058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Filter:** Filtra os elementos do RDD com base em uma condição booleana fornecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e590c15-7dbe-40bd-82f1-600cbe239c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Manter apenas os números pares\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(even_rdd.collect())  # Output: [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101ee42e-a8a4-4c30-a549-484606cc5256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **FlatMap:** Similar ao `map`, mas cada entrada do RDD pode ser mapeada para zero ou mais saídas. Resulta em um RDD \"achatado\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9724a752-d5de-4bd4-a1ee-09dab5603585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Dividir frases em palavras\n",
    "rdd = sc.parallelize([\"Hello World\", \"Apache Spark\"])\n",
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_rdd.collect())  # Output: ['Hello', 'World', 'Apache', 'Spark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fae6c5-e1b9-4cf0-8d84-8b3abf126e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **GroupByKey:** Agrupa os elementos de um RDD (que é uma coleção de pares chave-valor) pela chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bada530a-ecf5-45f8-8492-5f7910c587c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Agrupar números por seu valor de chave\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "grouped_rdd = rdd.groupByKey()\n",
    "print([(x, list(y)) for x, y in grouped_rdd.collect()])  # Output: [('a', [1, 3]), ('b', [2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009c6a95-1019-4893-8f3f-03d184609152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **ReduceByKey:** Semelhante ao `groupByKey`, mas aplica uma função de redução aos valores de cada chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7755dc8-35b5-4a0b-aa98-85e4b153789f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Somar valores por chave\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "summed_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(summed_rdd.collect())  # Output: [('a', 4), ('b', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f75ea7d-add4-4625-8aea-2327b1860998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Join:** Faz a junção de dois RDDs de pares chave-valor com base nas chaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cae602-87f5-46a0-aa60-0a353a72e491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Juntar dois RDDs com base na chave\n",
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "rdd2 = sc.parallelize([(\"a\", 3), (\"b\", 4)])\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "print(joined_rdd.collect())  # Output: [('a', (1, 3)), ('b', (2, 4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d04bfe-e32f-48a3-b34d-44772b0fcd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Distinct:** Retorna um novo RDD contendo apenas elementos únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b520519-dfd3-4a37-9a28-1d3e04884009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Remover elementos duplicados\n",
    "rdd = sc.parallelize([1, 2, 2, 3, 4, 4, 5])\n",
    "distinct_rdd = rdd.distinct()\n",
    "print(distinct_rdd.collect())  # Output: [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2806b349-4252-4f13-83ef-eae25b78cea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Sample:** Amostra um subconjunto dos elementos do RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0338cd5b-44fa-4a60-b71c-c249d72f469b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: Amostrar 50% dos elementos do RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "sample_rdd = rdd.sample(False, 0.5)\n",
    "print(sample_rdd.collect())  # Output: Pode variar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9a65b9-c71e-470b-b408-0abcdd9d8d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Ações\n",
    "- **Ações** são operações que desencadeiam a execução de todas as transformações acumuladas em um RDD e retornam um resultado ao driver ou escrevem dados para o armazenamento. Exemplos incluem `collect()`, `count()`, `first()`, `take(n)`, e `saveAsTextFile()`.\n",
    "\n",
    "##### DataFrames\n",
    "- Um **DataFrame** é uma coleção distribuída de dados organizada em colunas, muito similar a uma tabela em um banco de dados relacional. Eles são construídos sobre RDDs e trazem otimizações adicionais, como o Catalyst Optimizer para planos de consulta eficientes.\n",
    "\n",
    "##### Datasets\n",
    "- Os Datasets combinam as vantagens dos RDDs e DataFrames, proporcionando uma API orientada a objetos e uma maior eficiência. Eles são fortemente tipados e garantem segurança de tipo em tempo de compilação. Operações comuns incluem `map`, `filter`, `join`, e `groupBy`, com garantias de tipo.\n",
    "\n",
    "#### 1.3. Diferença entre Processamento em Lote e Tempo Real\n",
    "\n",
    "- **Processamento em Lote:** Refere-se ao processamento de grandes volumes de dados acumulados ao longo do tempo. Spark é altamente eficiente para este tipo de processamento usando RDDs e DataFrames.\n",
    "\n",
    "- **Processamento em Tempo Real:** Com Spark Streaming, o Spark pode processar dados em tempo real dividindo os fluxos de dados em pequenos lotes chamados micro-batches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada0664e-fe10-4ea5-92ac-5cc9933b8179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`[INFO]: FIM DO NOTEBOOK`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01.fundamentos-spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
