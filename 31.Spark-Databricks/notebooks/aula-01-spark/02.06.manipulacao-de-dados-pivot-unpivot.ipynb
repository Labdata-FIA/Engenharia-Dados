{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393f9eda-492d-4a12-ac56-ae2c5d569ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulação de Dados com DataFrames no PySpark - PIVOT E UNPIVOT\n",
    "\n",
    "#### Exemplo de Pivot no PySpark\n",
    "\n",
    "Documentação de referência: https://spark.apache.org/docs/3.5.2/sql-ref-syntax-qry-select-pivot.html\n",
    "\n",
    "- A cláusula `PIVOT` é utilizada para transformar os valores distintos de uma coluna (`category`) em colunas, aplicando uma função agregada como `SUM` para preencher os valores dessas novas colunas.\n",
    "\n",
    "##### Preparando os dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1888c3-5662-4000-9480-afb4cb562ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0016982-7963-4fb4-a1c8-3aaa42d9c263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Criar um DataFrame de exemplo\n",
    "data = [(\"2024-01\", \"A\", 10),\n",
    "        (\"2024-01\", \"B\", 20),\n",
    "        (\"2024-02\", \"A\", 30),\n",
    "        (\"2024-02\", \"B\", 40),\n",
    "        (\"2024-02\", \"C\", 50)]\n",
    "\n",
    "schema = [\"month\", \"category\", \"value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Mostrar o DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec04da5-e2d5-465c-9320-d9ba7aa2417d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Aplicando o Pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08349270-d1b5-46f1-81bf-dd9e992ec285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar Pivot para transformar categorias em colunas e somar os valores\n",
    "df_pivot = (\n",
    "    df\n",
    "        .groupBy(\"month\")\n",
    "        .pivot(\"category\")\n",
    "        .agg(sum(\"value\"))\n",
    ")\n",
    "df_pivot.createOrReplaceTempView(\"df_pivoted\")\n",
    "\n",
    "print(\"DataFrame Após Pivot:\")\n",
    "display(df_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e0923f-36ed-4485-a016-33078bb430c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Equivalente em SQL com PIVOT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e745ad7-6b6b-45e1-a918-04620bad3998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM df\n",
    "PIVOT (\n",
    "    SUM(value)\n",
    "    FOR category IN ('A' AS A, 'B' AS B, 'C' AS C)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6395ec-3677-448c-a056-d3065a42bc2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exemplo de Unpivot\n",
    "\n",
    "Documentação de referência: https://spark.apache.org/docs/3.5.2/sql-ref-syntax-qry-select-unpivot.html\n",
    "\n",
    "- A cláusula `UNPIVOT` faz o inverso, convertendo colunas em linhas. No exemplo, as colunas `A`, `B`, e `C` são transformadas em linhas, com a nova coluna `category` indicando o nome original da coluna, e `value` mantendo os dados correspondentes.\n",
    "\n",
    "#### Equivalente em SQL com UNPIVOT\n",
    "\n",
    "Para realizar o `UNPIVOT` diretamente em SQL, você pode usar a cláusula `UNPIVOT`, como demonstrado abaixo:\n",
    "\n",
    "\n",
    "**Aplicando UNPIVOT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9389ed90-a68e-4fce-bb85-e5269407dbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unpivot_df = spark.sql(\"\"\"\n",
    "    SELECT month, category, value\n",
    "    FROM df_pivoted\n",
    "    UNPIVOT (\n",
    "      value FOR category IN (A, B, C)\n",
    "  )\"\"\")\n",
    "display(\n",
    "  unpivot_df\n",
    "    .orderBy(\"month\", \"category\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7352d962-4628-4f37-8889-28feac54b8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`[INFO]: FIM DO NOTEBOOK`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4198194417609997,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02.06.manipulacao-de-dados-pivot-unpivot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
